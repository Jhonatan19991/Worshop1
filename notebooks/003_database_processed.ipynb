{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Processed by Jhonatan Steven Morales\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we will process the data to normalize it before importing it into the database while preserving the relationships between the tables. The tables will be named CandidatesProcessed and Countries, and we will make all necessary changes to ensure proper data transformation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensure that you already have your own .env file containing your environment variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "work_dir = os.getenv('WORK_DIR')\n",
    "\n",
    "sys.path.append(work_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "libraries & Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import inspect\n",
    "from sqlalchemy.orm import sessionmaker\n",
    "from  src.database.dbconnection import getconnection\n",
    "from python_code.transform import DataTransform\n",
    "from src.model.models import CandidatesProcessed\n",
    "from src.model.models import Countries\n",
    "from sqlalchemy.exc import SQLAlchemyError\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the SQLAlchemy library, connect to the database. If you encounter any issues, check that your .env file contains the correct environment variables and try again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conected successfully to database workshop1!\n"
     ]
    }
   ],
   "source": [
    "engine = getconnection()\n",
    "Session = sessionmaker(bind=engine)\n",
    "session = Session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure to create the countries table first, as it serves as the foreign key for the candidates table. This will help avoid any potential errors. In this process, ensure that there are no other tables with the same name. If such tables exist, they should be dropped before creating the new ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table created successfully.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    if inspect(engine).has_table('Countries'):\n",
    "        Countries.__table__.drop(engine)\n",
    "    Countries.__table__.create(engine)\n",
    "    print(\"Table created successfully.\")\n",
    "except SQLAlchemyError as e:\n",
    "    print(f\"Error creating table: {e}\")\n",
    "finally:\n",
    "    engine.dispose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table created successfully.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    if inspect(engine).has_table('CandidatesProcessed'):\n",
    "        CandidatesProcessed.__table__.drop(engine)\n",
    "    CandidatesProcessed.__table__.create(engine)\n",
    "    print(\"Table created successfully.\")\n",
    "except SQLAlchemyError as e:\n",
    "    print(f\"Error creating table: {e}\")\n",
    "finally:\n",
    "    engine.dispose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the previously created DataTransform class to perform the necessary transformations. First, we will ensure that each record is assigned a unique id. Next, we will add a new column, which will be a boolean variable where 0 indicates not hired and 1 indicates hired. We will rename columns that contain spaces. Finally, to simplify the technology field, we will group the different job types into predefined categories to make data analysis and visualization easier.\n",
    "\n",
    "Generalization of Categories\n",
    "\n",
    "Software Development:\n",
    "\n",
    "*   Game Development\n",
    "*   Development - Backend\n",
    "*   Development - FullStack\n",
    "*   Adobe Experience Manager\n",
    "*   Development - CMS Frontend\n",
    "*   Development - Frontend\n",
    "*   Development - CMS Backend\n",
    "\n",
    "DevOps and System Administration:\n",
    "\n",
    "*   DevOps\n",
    "*   System Administration\n",
    "*   Database Administration\n",
    "\n",
    "Management and Support:\n",
    "*   Social Media Community Management\n",
    "*   Client Success\n",
    "*   Sales\n",
    "\n",
    "Other Areas:\n",
    "\n",
    "*   Mulesoft\n",
    "*   Technical Writing\n",
    "*   Salesforce\n",
    "Data Engineering and Analytics:\n",
    "\n",
    "*   Data Engineer\n",
    "*   Business Intelligence\n",
    "*   Business Analytics / Project Management\n",
    "Security:\n",
    "\n",
    "*   Security\n",
    "*   Security Compliance\n",
    "Design and QA:\n",
    "\n",
    "*   Design\n",
    "*   QA Manual\n",
    "*   QA Automation\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After these changes, we will make minor adjustments to the data. Finally, we will separate the countries by a unique id, insert the countries into their respective table, and insert the candidates into the other table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data uploaded\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    transform_candidates = DataTransform('../data/candidates.csv')\n",
    "    transform_candidates.insert_id()\n",
    "    transform_candidates.HiredOrNotHired()\n",
    "    transform_candidates.rename()\n",
    "    transform_candidates.technology_by_category()\n",
    "    transform_candidates.FullNameReplace()\n",
    "    transform_candidates.ApplicationDateToDateType()\n",
    "    \n",
    "     \n",
    "    transform_countrys = transform_candidates.NormalizeCountry()\n",
    "    \n",
    "    transform_countrys.to_sql('Countries', con=engine, if_exists='append', index=False)\n",
    "    transform_candidates.df.to_sql('CandidatesProcessed', con=engine, if_exists='append', index=False)\n",
    "    \n",
    "    print(\"Data uploaded\")\n",
    "\n",
    "except SQLAlchemyError as e:\n",
    "    print(f\"Database error: {e}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "\n",
    "finally:\n",
    "    if hasattr(engine, 'dispose'):\n",
    "        engine.dispose()\n",
    "    if 'session' in locals():\n",
    "        session.close()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
